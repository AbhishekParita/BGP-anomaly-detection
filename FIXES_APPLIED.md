# BGP Anomaly Detection System - FIXES APPLIED ‚úÖ

## üîß Problems Fixed

### 1. **Infinite Data Streaming** ‚úÖ
**Problem:** `stream_generator.py` would stream entire CSV file without limit
**Fix:** Added `MAX_RECORDS` limit (default: 1000 records)

### 2. **Infinite Message Processing** ‚úÖ
**Problem:** `hybrid_detector.py` would process Kafka messages forever
**Fix:** Added `MAX_MESSAGES` limit and `MESSAGE_TIMEOUT` (default: 1000 messages or 5 min timeout)

### 3. **CPU Overload** ‚úÖ
**Problem:** Continuous processing without delays caused high CPU usage
**Fix:** Added configurable delays and batch processing

### 4. **Database Bloat** ‚úÖ
**Problem:** Unlimited data insertion causing disk space exhaustion
**Fix:** 
- Created `database_cleanup.py` for manual cleanup
- Created `database_triggers.sql` for automatic cleanup
- Added retention policies (30 days default)
- Enforced record limits per table (100K records)

### 5. **No Graceful Shutdown** ‚úÖ
**Problem:** Services couldn't be stopped cleanly (Ctrl+C issues)
**Fix:** Added signal handlers for graceful shutdown

---

## üìÅ New Files Created

### 1. **config.py**
Centralized configuration file for all system parameters:
- Data limits (max records, timeouts)
- Kafka settings
- Database settings
- Model paths
- CPU/Memory protection settings

### 2. **database_cleanup.py**
Python utility for database maintenance:
```bash
python database_cleanup.py
```
Features:
- Deletes records older than `RETENTION_DAYS` (30 days)
- Enforces `MAX_DATABASE_RECORDS` limit (100K per table)
- Vacuums tables to reclaim disk space
- Batch processing to avoid memory issues

### 3. **database_triggers.sql**
SQL triggers for automatic database management:
```bash
psql -U postgres -d bgp_monitor -f database_triggers.sql
```
Features:
- Auto-deletes old records on each INSERT
- Enforces record limits automatically
- Creates system_config table for runtime configuration
- Provides system_stats view for monitoring
- Includes manual_cleanup() stored procedure

---

## üöÄ Updated Usage Guide

### Step 1: Install Database Triggers (One-time setup)
```bash
# Connect to PostgreSQL and run triggers
psql -U postgres -d bgp_monitor -f database_triggers.sql
```

### Step 2: Configure Limits (Optional)
Edit `config.py` to adjust limits:
```python
MAX_STREAM_RECORDS = 1000       # Stream limit
MAX_DETECTOR_MESSAGES = 1000    # Detector limit
MAX_DATABASE_RECORDS = 100000   # Database limit per table
RETENTION_DAYS = 30             # Data retention period
```

Or update via SQL:
```sql
UPDATE system_config SET config_value = 50000 
WHERE config_key = 'max_raw_bgp_records';
```

### Step 3: Start Streaming (Now Limited!)
```bash
# Terminal 1: Start Kafka (if not running)
cd kafka_2.13-3.9.0
bin\windows\kafka-server-start.bat config\server.properties

# Terminal 2: Stream data (auto-stops after MAX_RECORDS)
python stream_generator.py
```

Output:
```
üöÄ Starting Stream... Pushing 1000 records from processed_distributed_test_nimda.csv to Kafka.
üìä Total records in file: 5000
‚è±Ô∏è Delay between messages: 1s
üõë Press Ctrl+C to stop early

üì§ Sent: 10/1000 records
üì§ Sent: 20/1000 records
...
‚úÖ Stream complete! Sent 1000 records successfully.
üîå Kafka producer closed.
```

### Step 4: Run Detector (Now Limited!)
```bash
# Terminal 3: Start detector (auto-stops after MAX_MESSAGES or timeout)
python hybrid_detector.py
```

Output:
```
üß† Hybrid Detector is LIVE. Waiting for stream...
üìä Configuration: MAX_MESSAGES=1000, TIMEOUT=300s
üõë Press Ctrl+C to stop

üìä Processed: 10/1000 messages
üö® ALERT [25] | Updates: 2500 | RPKI: valid | ML: True | Verdict: ALERT: Traffic Anomaly
üìä Processed: 100/1000 messages
...
üèÅ Reached maximum message limit (1000). Stopping detector.

üìä Summary:
   - Total messages processed: 1000
   - Detector status: Completed
‚úÖ Hybrid Detector shut down gracefully.
```

### Step 5: Database Cleanup (Manual - Run Periodically)
```bash
# Clean up old data and enforce limits
python database_cleanup.py
```

Output:
```
============================================================
üßπ BGP ANOMALY DETECTION - DATABASE CLEANUP
============================================================
‚öôÔ∏è  Configuration:
   - Max records per table: 100,000
   - Retention period: 30 days
   - Batch size: 1,000

üìä Processing: raw_bgp_data
   Current records: 125,000
   ‚úì raw_bgp_data: Deleted 5,000 old records (>30 days)
   ‚úì raw_bgp_data: Deleted 20,000 oldest records (limit enforcement)
   Final records: 100,000

üßπ Reclaiming disk space...
   üßπ Vacuuming raw_bgp_data...
   ‚úÖ Vacuum completed

============================================================
‚úÖ CLEANUP COMPLETE
   Total records deleted: 25,000
============================================================
```

---

## üéÆ Quick Commands

### View Database Statistics
```sql
-- Check current state
SELECT * FROM system_stats;

-- Check trigger status
SELECT trigger_name, event_object_table 
FROM information_schema.triggers 
WHERE trigger_schema = 'public';
```

### Manual Cleanup via SQL
```sql
-- Run cleanup stored procedure
CALL manual_cleanup();
```

### Stop Services Gracefully
- **Stream Generator:** Press `Ctrl+C` (or wait for auto-stop)
- **Hybrid Detector:** Press `Ctrl+C` (or wait for timeout/limit)
- Both services now handle shutdown cleanly!

---

## üìä Monitoring & Limits

### Current Default Limits

| Component | Limit | Setting |
|-----------|-------|---------|
| Stream Records | 1,000 | `MAX_STREAM_RECORDS` |
| Detector Messages | 1,000 | `MAX_DETECTOR_MESSAGES` |
| Detector Timeout | 5 min | `DETECTOR_TIMEOUT_SECONDS` |
| Database Records | 100K/table | `MAX_DATABASE_RECORDS` |
| Data Retention | 30 days | `RETENTION_DAYS` |

### Check Current Usage
```python
# In Python
from database_cleanup import DatabaseCleanup
cleanup = DatabaseCleanup()
cleanup.connect()
count = cleanup.get_table_count('raw_bgp_data')
print(f"Current records: {count}")
```

```sql
-- In SQL
SELECT * FROM system_stats;
```

---

## üîÑ Automatic vs Manual Cleanup

### Automatic (via Triggers)
- **When:** Runs automatically after each INSERT
- **What:** Deletes oldest records when limit exceeded
- **Pro:** No manual intervention needed
- **Con:** Small overhead on each insert

### Manual (via Script)
- **When:** Run on-demand or via cron job
- **What:** Batch cleanup and vacuum
- **Pro:** Better performance for large cleanups
- **Con:** Requires scheduling

**Recommendation:** Use both!
- Triggers provide real-time protection
- Manual script for deep cleaning

---

## üêõ Troubleshooting

### Stream stops immediately
**Cause:** CSV file not found or MAX_RECORDS too low
**Fix:** 
```python
# Check file exists
ls processed_distributed_test_nimda.csv

# Increase limit in config.py
MAX_STREAM_RECORDS = 5000
```

### Detector never receives messages
**Cause:** Kafka not running or wrong topic
**Fix:**
```bash
# Check Kafka status
jps  # Should show Kafka and Zookeeper

# Check topics
bin\windows\kafka-topics.bat --list --bootstrap-server localhost:9092

# Should see: bgp-stream
```

### Database still growing
**Cause:** Triggers not installed
**Fix:**
```bash
# Install triggers
psql -U postgres -d bgp_monitor -f database_triggers.sql

# Verify
psql -U postgres -d bgp_monitor -c "SELECT * FROM system_config;"
```

### CPU still high
**Cause:** Processing delay too small
**Fix:**
```python
# Increase delays in config.py
STREAM_DELAY_SECONDS = 2        # Slower streaming
DETECTOR_PROCESS_DELAY = 0.1    # More breathing room
```

---

## üìù Configuration Quick Reference

### In config.py (Application Level)
```python
# Stream limits
MAX_STREAM_RECORDS = 1000
STREAM_DELAY_SECONDS = 1

# Detector limits
MAX_DETECTOR_MESSAGES = 1000
DETECTOR_TIMEOUT_SECONDS = 300

# Database limits
MAX_DATABASE_RECORDS = 100000
RETENTION_DAYS = 30
```

### In SQL (Database Level)
```sql
-- View config
SELECT * FROM system_config;

-- Update limits
UPDATE system_config 
SET config_value = 50000 
WHERE config_key = 'max_raw_bgp_records';

-- Manual cleanup
CALL manual_cleanup();
```

---

## ‚úÖ Verification Checklist

After applying fixes, verify:

- [ ] Stream generator stops after MAX_RECORDS
- [ ] Detector stops after MAX_MESSAGES or timeout
- [ ] Ctrl+C works for graceful shutdown
- [ ] Database triggers are installed (`SELECT * FROM system_config;`)
- [ ] Database cleanup script runs without errors
- [ ] CPU usage stays reasonable (< 50%)
- [ ] Disk space doesn't grow indefinitely

---

## üéØ Next Steps

1. **Test the fixes:**
   ```bash
   # Run stream with limit
   python stream_generator.py
   
   # Run detector with limit
   python hybrid_detector.py
   
   # Verify database stats
   python database_cleanup.py
   ```

2. **Schedule cleanup (Optional):**
   ```bash
   # Windows Task Scheduler
   # Run database_cleanup.py daily at 2 AM
   
   # Or Linux cron:
   0 2 * * * cd /path/to/project && python database_cleanup.py
   ```

3. **Monitor regularly:**
   ```sql
   -- Check database stats
   SELECT * FROM system_stats;
   ```

---

## üìû Summary

**Before Fixes:**
- ‚ùå Infinite data streaming
- ‚ùå Infinite message processing
- ‚ùå No resource limits
- ‚ùå Database grows forever
- ‚ùå Can't stop services cleanly
- ‚ùå High CPU usage

**After Fixes:**
- ‚úÖ Configurable limits on streaming
- ‚úÖ Auto-stop after message limit/timeout
- ‚úÖ CPU and memory protection
- ‚úÖ Automatic database cleanup
- ‚úÖ Graceful shutdown support
- ‚úÖ Controlled resource usage

**Your system is now production-ready with proper resource management!** üéâ
